"""
æ¡é›†å™¨æ•´åˆæ¨¡çµ„ - æ•´åˆæ‰€æœ‰è³‡æ–™ä¾†æºä¸¦è¼¸å‡ºæ¨™æº–åŒ– JSON
"""

import json
import logging
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Optional

from .binance import BinanceClient, FuturesDerivativesData, OHLCData
from .cache import OHLCCache
from .coingecko import BTCPriceData, CoinGeckoClient, GlobalMarketData
from .fear_greed import FearGreedClient, FearGreedData
from .news import NewsClient, NewsItem
from .technical import TechnicalAnalyzer, TechnicalIndicators

logger = logging.getLogger(__name__)


@dataclass
class DailyContext:
    """æ¯æ—¥å¸‚å ´è³‡æ–™å®Œæ•´ä¸Šä¸‹æ–‡"""

    collected_at: str  # è³‡æ–™æ¡é›†æ™‚é–“
    price: dict  # BTC åƒ¹æ ¼è³‡æ–™
    sentiment: dict  # ææ…Œè²ªå©ªæŒ‡æ•¸
    news: list[dict]  # æ–°èåˆ—è¡¨
    technical: dict = field(default_factory=dict)  # æŠ€è¡“æŒ‡æ¨™
    market_structure: dict = field(default_factory=dict)  # å¸‚å ´çµæ§‹ (BTC Dominance ç­‰)
    derivatives: Optional[dict] = None  # ç±Œç¢¼é¢æŒ‡æ¨™ (OI, å¤šç©ºæ¯”, äº¤æ˜“æ‰€æµé‡)
    metadata: dict = field(default_factory=dict)  # å…ƒè³‡æ–™

    def to_dict(self) -> dict:
        return {
            "collected_at": self.collected_at,
            "price": self.price,
            "sentiment": self.sentiment,
            "news": self.news,
            "technical": self.technical,
            "market_structure": self.market_structure,
            "derivatives": self.derivatives,
            "metadata": self.metadata,
        }

    def to_json(self, indent: int = 2) -> str:
        return json.dumps(self.to_dict(), indent=indent, ensure_ascii=False)


class Collector:
    """
    è³‡æ–™æ¡é›†å™¨ - æ•´åˆ CoinGeckoã€Fear & Greed Indexã€Google Newsã€Binance Kç·š/æœŸè²¨ã€æŠ€è¡“æŒ‡æ¨™

    ä½¿ç”¨æ–¹å¼:
        collector = Collector()
        context = collector.collect_all()
        collector.save_to_file(context, "data/daily_context.json")
    """

    def __init__(
        self,
        coingecko_api_key: Optional[str] = None,
        news_language: str = "en",
        news_country: str = "US",
        data_dir: str = "data",
    ):
        """
        åˆå§‹åŒ–æ¡é›†å™¨

        Args:
            coingecko_api_key: CoinGecko Pro API Key (å¯é¸)
            news_language: æ–°èèªè¨€
            news_country: æ–°èåœ‹å®¶
            data_dir: è³‡æ–™ç›®éŒ„ (ç”¨æ–¼ K ç·šå¿«å–)
        """
        self.coingecko = CoinGeckoClient(api_key=coingecko_api_key)
        self.fear_greed = FearGreedClient()
        self.news = NewsClient(language=news_language, country=news_country)
        self.binance = BinanceClient()
        self.cache = OHLCCache(data_dir=data_dir)
        self.technical_analyzer = TechnicalAnalyzer()

    def collect_price(self) -> BTCPriceData:
        """æ¡é›† BTC åƒ¹æ ¼è³‡æ–™"""
        return self.coingecko.get_btc_price()

    def collect_sentiment(self) -> FearGreedData:
        """æ¡é›†ææ…Œè²ªå©ªæŒ‡æ•¸"""
        return self.fear_greed.get_current_index()

    def collect_news(self, limit: int = 3, fetch_content: bool = True) -> list[NewsItem]:
        """
        æ¡é›†åŠ å¯†è²¨å¹£æ–°è (åŒ…å«æ–‡ç« å…§å®¹)
        
        Args:
            limit: æ¯å€‹æ–°èä¾†æºçš„æ–°èæ•¸é‡
            fetch_content: æ˜¯å¦çˆ¬å–æ–‡ç« å…¨æ–‡
        
        Returns:
            list[NewsItem]: æ–°èåˆ—è¡¨
        """
        return self.news.get_crypto_news_from_sources(
            sources=["coindesk", "cointelegraph"],
            limit=limit,
            fetch_content=fetch_content,
        )

    def collect_global_market(self) -> GlobalMarketData:
        """æ¡é›†å…¨çƒå¸‚å ´æ•¸æ“š (BTC Dominance)"""
        return self.coingecko.get_global_data()

    def collect_klines(self, interval: str = "1d", use_cache: bool = True) -> list[dict]:
        """
        æ¡é›† K ç·šæ•¸æ“š (æ”¯æ´å¿«å–)

        Args:
            interval: K ç·šé€±æœŸ (1d, 4h)
            use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–

        Returns:
            list[dict]: K ç·šæ•¸æ“šåˆ—è¡¨
        """
        if use_cache:
            # æª¢æŸ¥å¿«å–æ˜¯å¦éœ€è¦æ›´æ–°
            start_time = self.cache.get_update_start_time(interval)

            if start_time:
                # å¢é‡æ›´æ–°
                logger.info(f"å¾å¿«å–å¢é‡æ›´æ–° {interval} K ç·š...")
                new_klines = self.binance.get_klines(
                    interval=interval,
                    start_time=start_time,
                    limit=1000,
                )
                if new_klines:
                    new_klines_dict = [k.to_dict() for k in new_klines]
                    self.cache.save(interval, new_klines_dict, merge=True)
            else:
                # å®Œæ•´æŠ“å–
                logger.info(f"åˆå§‹åŒ– {interval} K ç·šå¿«å–...")
                days = 365 if interval == "1d" else 90
                klines = self.binance.get_daily_klines(days) if interval == "1d" else self.binance.get_4h_klines(days)
                klines_dict = [k.to_dict() for k in klines]
                self.cache.save(interval, klines_dict, merge=False)

            return self.cache.get_cached_klines(interval)
        else:
            # ä¸ä½¿ç”¨å¿«å–ï¼Œç›´æ¥æŠ“å–
            days = 365 if interval == "1d" else 90
            klines = self.binance.get_daily_klines(days) if interval == "1d" else self.binance.get_4h_klines(days)
            return [k.to_dict() for k in klines]

    def collect_technical(self, klines: list[dict] = None) -> TechnicalIndicators:
        """
        æ¡é›†æŠ€è¡“æŒ‡æ¨™

        Args:
            klines: K ç·šæ•¸æ“š (å¯é¸ï¼Œä¸æä¾›å‰‡è‡ªå‹•æ¡é›†)

        Returns:
            TechnicalIndicators: æŠ€è¡“æŒ‡æ¨™é›†åˆ
        """
        if klines is None:
            klines = self.collect_klines(interval="1d", use_cache=True)

        return self.technical_analyzer.calculate(klines)

    def collect_all(self, news_limit: int = 3) -> DailyContext:
        """
        æ¡é›†æ‰€æœ‰è³‡æ–™ä¾†æº

        Args:
            news_limit: æ–°èæ•¸é‡é™åˆ¶

        Returns:
            DailyContext: å®Œæ•´çš„æ¯æ—¥å¸‚å ´ä¸Šä¸‹æ–‡

        Raises:
            Exception: ä»»ä¸€è³‡æ–™ä¾†æºæ¡é›†å¤±æ•—æ™‚æ‹‹å‡º
        """
        logger.info("=" * 50)
        logger.info("é–‹å§‹æ¡é›†æ¯æ—¥å¸‚å ´è³‡æ–™...")
        logger.info("=" * 50)

        errors = []

        # æ¡é›†åƒ¹æ ¼è³‡æ–™
        price_data = None
        try:
            price_data = self.collect_price()
        except Exception as e:
            logger.error(f"åƒ¹æ ¼è³‡æ–™æ¡é›†å¤±æ•—: {e}")
            errors.append(f"price: {e}")

        # æ¡é›†æƒ…ç·’æŒ‡æ•¸
        sentiment_data = None
        try:
            sentiment_data = self.collect_sentiment()
        except Exception as e:
            logger.error(f"æƒ…ç·’æŒ‡æ•¸æ¡é›†å¤±æ•—: {e}")
            errors.append(f"sentiment: {e}")

        # æ¡é›†æ–°è
        news_data = []
        try:
            news_data = self.collect_news(limit=news_limit)
        except Exception as e:
            logger.error(f"æ–°èæ¡é›†å¤±æ•—: {e}")
            errors.append(f"news: {e}")

        # æ¡é›† K ç·šæ•¸æ“šä¸¦è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
        technical_data = None
        try:
            klines = self.collect_klines(interval="1d", use_cache=True)
            technical_data = self.collect_technical(klines)
        except Exception as e:
            logger.error(f"æŠ€è¡“æŒ‡æ¨™æ¡é›†å¤±æ•—: {e}")
            errors.append(f"technical: {e}")

        # æ¡é›†å…¨çƒå¸‚å ´æ•¸æ“š (BTC Dominance)
        global_market_data = None
        try:
            global_market_data = self.collect_global_market()
        except Exception as e:
            logger.error(f"å…¨çƒå¸‚å ´æ•¸æ“šæ¡é›†å¤±æ•—: {e}")
            errors.append(f"market_structure: {e}")

        # æ¡é›†ç±Œç¢¼é¢æŒ‡æ¨™ (OI, å¤šç©ºæ¯”, è³‡é‡‘è²»ç‡) - ä½¿ç”¨ Binance Futures
        derivatives_data = None
        try:
            derivatives_data = self.binance.get_derivatives_data()
        except Exception as e:
            logger.warning(f"ç±Œç¢¼é¢æŒ‡æ¨™æ¡é›†å¤±æ•— (éè‡´å‘½): {e}")
            errors.append(f"derivatives: {e}")

        # æª¢æŸ¥æ˜¯å¦æœ‰é—œéµè³‡æ–™ç¼ºå¤±
        if price_data is None or sentiment_data is None:
            raise RuntimeError(f"é—œéµè³‡æ–™æ¡é›†å¤±æ•—: {errors}")

        # çµ„è£å®Œæ•´ä¸Šä¸‹æ–‡
        context = DailyContext(
            collected_at=datetime.now().isoformat(),
            price=price_data.to_dict(),
            sentiment={
                **sentiment_data.to_dict(),
                "sentiment_zh": sentiment_data.sentiment_zh,
                "emoji": sentiment_data.sentiment_emoji,
            },
            news=[item.to_dict() for item in news_data],
            technical=technical_data.to_dict() if technical_data else {},
            market_structure=global_market_data.to_dict() if global_market_data else {},
            derivatives=derivatives_data.to_dict() if derivatives_data else None,
            metadata={
                "version": "2.1.0",
                "sources": {
                    "price": "CoinGecko",
                    "sentiment": "Alternative.me",
                    "news": "Google News RSS",
                    "klines": "Binance",
                    "technical": "pandas-ta",
                    "market_structure": "CoinGecko Global",
                    "derivatives": "Binance Futures" if derivatives_data else None,
                },
                "errors": errors if errors else None,
            },
        )

        logger.info("=" * 50)
        logger.info("è³‡æ–™æ¡é›†å®Œæˆï¼")
        logger.info(f"  åƒ¹æ ¼: ${price_data.price_usd:,.2f}")
        logger.info(f"  æƒ…ç·’: {sentiment_data.sentiment_zh} ({sentiment_data.value})")
        logger.info(f"  æ–°è: {len(news_data)} å‰‡")
        if technical_data:
            logger.info(f"  æŠ€è¡“: {technical_data.overall_signal_zh}")
        if global_market_data:
            logger.info(f"  BTC.D: {global_market_data.btc_dominance:.1f}%")
        if derivatives_data:
            logger.info(f"  ç±Œç¢¼: å·²æ¡é›† (OI/å¤šç©ºæ¯”/äº¤æ˜“æ‰€æµé‡)")
        logger.info("=" * 50)

        return context

    def save_to_file(self, context: DailyContext, filepath: str | Path) -> Path:
        """
        å°‡æ¡é›†è³‡æ–™ä¿å­˜ç‚º JSON æª”æ¡ˆ

        Args:
            context: æ¯æ—¥å¸‚å ´ä¸Šä¸‹æ–‡
            filepath: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘

        Returns:
            Path: å¯¦éš›ä¿å­˜çš„æª”æ¡ˆè·¯å¾‘
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(context.to_json())

        logger.info(f"è³‡æ–™å·²ä¿å­˜è‡³: {filepath}")
        return filepath

    def collect_multi_day(
        self,
        days: int = 3,
        news_limit_per_day: int = 3,
        include_today: bool = True,
    ) -> list[DailyContext]:
        """
        æ¡é›†å¤šæ—¥å¸‚å ´è³‡æ–™ï¼ˆç”¨æ–¼ç¶œåˆæŠ•è³‡å ±å‘Šï¼‰

        Args:
            days: è¦æ¡é›†çš„å¤©æ•¸ï¼ˆåŒ…å«ä»Šå¤©ï¼‰
            news_limit_per_day: æ¯å¤©çš„æ–°èæ•¸é‡é™åˆ¶
            include_today: æ˜¯å¦åŒ…å«ä»Šå¤©çš„å³æ™‚è³‡æ–™

        Returns:
            list[DailyContext]: æŒ‰æ—¥æœŸæ’åºçš„æ¯æ—¥å¸‚å ´ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ˆæœ€èˆŠåˆ°æœ€æ–°ï¼‰
        """
        from datetime import date, timedelta
        
        logger.info("=" * 50)
        logger.info(f"é–‹å§‹æ¡é›†éå» {days} å¤©çš„å¸‚å ´è³‡æ–™...")
        logger.info("=" * 50)
        
        contexts = []
        today = date.today()
        
        # 1. é¦–å…ˆæ¡é›†ä»Šå¤©çš„å³æ™‚è³‡æ–™
        if include_today:
            try:
                logger.info(f"[1/{days}] æ¡é›†ä»Šæ—¥ ({today}) å³æ™‚è³‡æ–™...")
                today_context = self.collect_all(news_limit=news_limit_per_day)
                contexts.append((today, today_context))
            except Exception as e:
                logger.error(f"ä»Šæ—¥è³‡æ–™æ¡é›†å¤±æ•—: {e}")
        
        # 2. å¾å¿«å–ä¸­æå–æ­·å² K ç·šæ•¸æ“š
        logger.info("è¼‰å…¥æ­·å² K ç·šå¿«å–...")
        cached_klines = self.cache.get_cached_klines("1d")
        klines_by_date = {}
        for kline in cached_klines:
            # æ”¯æ´å…©ç¨®æ ¼å¼: 'datetime' (æ–°æ ¼å¼) æˆ– 'open_time' (èˆŠæ ¼å¼)
            time_key = kline.get("datetime") or kline.get("open_time")
            if time_key:
                try:
                    kline_date = datetime.fromisoformat(time_key.replace("Z", "+00:00")).date()
                    klines_by_date[kline_date] = kline
                except Exception:
                    pass
        
        # 3. æ¡é›†æ­·å²æ–°è
        historical_days = days - 1 if include_today else days
        today_news_as_fallback = []  # å¦‚æœæ­·å²æ–°èä¸å¯ç”¨ï¼Œä½¿ç”¨ä»Šå¤©çš„æ–°èä½œç‚ºæ›¿ä»£
        
        if historical_days > 0:
            start_date = today - timedelta(days=historical_days)
            end_date = today - timedelta(days=1)
            
            logger.info(f"æ¡é›† {start_date} è‡³ {end_date} çš„æ­·å²æ–°è...")
            try:
                historical_news = self.news.get_historical_news_batch(
                    start_date=start_date,
                    end_date=end_date,
                    limit_per_day=news_limit_per_day,
                    delay_between_days=1.0,
                )
                
                # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰æ­·å²æ—¥æœŸéƒ½æ²’æœ‰æ–°è
                total_historical_news = sum(len(v) for v in historical_news.values())
                if total_historical_news == 0:
                    logger.warning("æ­·å²æ–°èç²å–ç‚ºç©ºï¼Œå°‡ä½¿ç”¨ä»Šæ—¥æ–°èä½œç‚ºæ›¿ä»£")
                    # ä½¿ç”¨ä»Šå¤©çš„æ–°èä½œç‚º fallback
                    if contexts and contexts[0][1].news:
                        today_news_as_fallback = contexts[0][1].news
                        logger.info(f"å°‡ä½¿ç”¨ {len(today_news_as_fallback)} å‰‡ä»Šæ—¥æ–°èä½œç‚ºæ­·å²åƒè€ƒ")
                    
            except Exception as e:
                logger.warning(f"æ­·å²æ–°èæ¡é›†å¤±æ•—: {e}")
                historical_news = {}
            
            # 4. ç‚ºæ¯å€‹æ­·å²æ—¥æœŸçµ„è£ DailyContext
            for i in range(historical_days, 0, -1):
                target_date = today - timedelta(days=i)
                date_str = target_date.strftime("%Y-%m-%d")
                
                logger.info(f"çµ„è£ {date_str} çš„å¸‚å ´ä¸Šä¸‹æ–‡...")
                
                try:
                    # å¾å¿«å–å–å¾—è©²æ—¥çš„ K ç·šæ•¸æ“š
                    kline = klines_by_date.get(target_date)
                    if kline:
                        # ç”¨ K ç·šæ•¸æ“šæ§‹å»ºåƒ¹æ ¼è³‡è¨Š
                        price_data = {
                            "price_usd": kline["close"],
                            "change_24h": ((kline["close"] - kline["open"]) / kline["open"]) * 100 if kline["open"] > 0 else 0,
                            "volume_24h": kline["volume"],
                            "high_24h": kline["high"],
                            "low_24h": kline["low"],
                            "market_cap": 0,  # æ­·å²è³‡æ–™ç„¡æ³•å–å¾—
                        }
                    else:
                        logger.warning(f"æ‰¾ä¸åˆ° {date_str} çš„ K ç·šè³‡æ–™")
                        price_data = {"price_usd": 0, "change_24h": 0}
                    
                    # å–å¾—è©²æ—¥æ–°è (å¦‚æœæ²’æœ‰æ­·å²æ–°èï¼Œä½¿ç”¨ä»Šæ—¥æ–°èä½œç‚ºåƒè€ƒ)
                    day_news = historical_news.get(date_str, [])
                    if day_news:
                        news_list = [item.to_dict() for item in day_news]
                    elif today_news_as_fallback:
                        # ä½¿ç”¨ä»Šå¤©çš„æ–°èä½†æ¨™è¨˜ç‚ºåƒè€ƒ
                        news_list = today_news_as_fallback.copy()
                        for n in news_list:
                            n["_note"] = "ä½¿ç”¨ä»Šæ—¥æ–°èä½œç‚ºæ­·å²åƒè€ƒ"
                    else:
                        news_list = []
                    
                    # è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ï¼ˆä½¿ç”¨åˆ°è©²æ—¥ç‚ºæ­¢çš„ K ç·šï¼‰
                    def get_kline_date(k):
                        time_key = k.get("datetime") or k.get("open_time")
                        if time_key:
                            try:
                                return datetime.fromisoformat(time_key.replace("Z", "+00:00")).date()
                            except Exception:
                                return None
                        return None
                    
                    historical_klines = [
                        k for k in cached_klines 
                        if get_kline_date(k) is not None and get_kline_date(k) <= target_date
                    ]
                    technical_data = {}
                    if len(historical_klines) >= 50:
                        try:
                            indicators = self.technical_analyzer.calculate(historical_klines[-200:])
                            technical_data = indicators.to_dict()
                        except Exception as e:
                            logger.warning(f"æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å¤±æ•—: {e}")
                    
                    # çµ„è£ DailyContext
                    historical_context = DailyContext(
                        collected_at=f"{date_str}T23:59:59",
                        price=price_data,
                        sentiment={"value": 50, "label": "Neutral", "sentiment_zh": "ä¸­æ€§", "emoji": "ğŸ˜"},  # æ­·å²æƒ…ç·’ç„¡æ³•å–å¾—
                        news=news_list,
                        technical=technical_data,
                        market_structure={},
                        derivatives=None,
                        metadata={
                            "version": "2.1.0",
                            "type": "historical",
                            "date": date_str,
                        },
                    )
                    
                    contexts.append((target_date, historical_context))
                    
                except Exception as e:
                    logger.error(f"çµ„è£ {date_str} ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
        
        # 5. æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€èˆŠåˆ°æœ€æ–°ï¼‰
        contexts.sort(key=lambda x: x[0])
        result = [ctx for _, ctx in contexts]
        
        logger.info("=" * 50)
        logger.info(f"å¤šæ—¥è³‡æ–™æ¡é›†å®Œæˆï¼å…± {len(result)} å¤©")
        for target_date, ctx in contexts:
            price = ctx.price.get("price_usd", 0)
            news_count = len(ctx.news)
            logger.info(f"  {target_date}: ${price:,.2f} | {news_count} å‰‡æ–°è")
        logger.info("=" * 50)
        
        return result


if __name__ == "__main__":
    # æ¸¬è©¦ç”¨
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    collector = Collector()
    context = collector.collect_all()
    print("\n" + "=" * 50)
    print("å®Œæ•´ JSON è¼¸å‡º:")
    print("=" * 50)
    print(context.to_json())
